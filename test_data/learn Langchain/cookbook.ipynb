{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os \n",
    "openai_api_key = os.environ.get('OPENAI_API_KEY')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "testing model 3.5 turbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import OpenAI\n",
    "llm = OpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.3)\n",
    "text = input(\"Enter text: \")\n",
    "print(llm(text))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chat Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a model that take series of messages and return a message output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage, AIMessage, SystemMessage\n",
    "\n",
    "chat = ChatOpenAI(temperature=1, openai_api_key=openai_api_key)\n",
    "chat([\n",
    "    SystemMessage(content=\"You are a gross bot that makes every word gross.\"),\n",
    "    HumanMessage(content=\"Is Benzema going to Al Ittihad?\")\n",
    "])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(model_name=\"gpt-3.5-turbo\",temperature=0.1)\n",
    "\n",
    "chat = \"\"\"\n",
    "Where is {football_player} playing right now?\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"football_player\"],\n",
    "    template = chat\n",
    ")\n",
    "\n",
    "text = prompt.format(football_player=input(\"Enter football player: \"))\n",
    "print(llm(text))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example Selectors:\n",
    "select a series of examples that allow you to dynamic place in-context info into your prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.example_selector import SemanticSimilarityExampleSelector\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.prompts import PromptTemplate, FewShotPromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(openai_api_key=openai_api_key,temperature=0.1, model_name=\"gpt-3.5-turbo-0613\")\n",
    "\n",
    "# give example \n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"football_player\", \"team\"],\n",
    "    template = \"Example Football Player: {football_player}\\nCurrent Team: {team}\"\n",
    ")\n",
    "\n",
    "examples = [\n",
    "    {\"football_player\": \"Benzema\", \"team\": \"Real Madrid\"},\n",
    "    {\"football_player\": \"Messi\", \"team\": \"Barcelona\"},\n",
    "    {\"football_player\": \"Ronaldo\", \"team\": \"Juventus\"},\n",
    "    {\"football_player\": \"Mbappe\", \"team\": \"PSG\"},\n",
    "    {\"football_player\": \"Neymar\", \"team\": \"PSG\"},\n",
    "    {\"football_player\": \"Haaland\", \"team\": \"Dortmund\"},\n",
    "    {\"football_player\": \"Lewandowski\", \"team\": \"Bayern Munich\"},\n",
    "    {\"football_player\": \"Kane\", \"team\": \"Tottenham\"},\n",
    "    {\"football_player\": \"Salah\", \"team\": \"Liverpool\"},\n",
    "    {\"football_player\": \"Beckham\", \"team\": \"Retired\"},\n",
    "    {\"football_player\": \"Zidane\", \"team\": \"Retired\"},\n",
    "    {\"football_player\": \"Ronaldinho\", \"team\": \"Retired\"},\n",
    "    {\"football_player\": \"Cafu\", \"team\": \"Retired\"},\n",
    "]\n",
    "\n",
    "# SemanticSimilarityExampleSelector will select example that are similar to input\n",
    "example_selector = SemanticSimilarityExampleSelector.from_examples(\n",
    "\n",
    "    # list of examples available to select from \n",
    "    examples,\n",
    "    \n",
    "    # embedding\n",
    "    OpenAIEmbeddings(openai_api_key=openai_api_key),\n",
    "    \n",
    "    # vector store embeddings and do a similar\n",
    "    FAISS,\n",
    "    \n",
    "    # how many we want\n",
    "    k=1,\n",
    ")\n",
    "\n",
    "similar_prompt = FewShotPromptTemplate(\n",
    "    # this will help select examples\n",
    "    example_selector=example_selector,\n",
    "    example_prompt=example_prompt,\n",
    "\n",
    "    # customize that will be added to the top of bottom of your prompt\n",
    "    prefix=\"give the club of football player plays in\",\n",
    "    suffix=\"football_player: {football_player}\\nteam:\",\n",
    "\n",
    "    # what inputs your prompt will receive\n",
    "\n",
    "    input_variables=[\"football_player\"],\n",
    ")\n",
    "\n",
    "my_player = input(\"Enter football player: \")\n",
    "result = llm(similar_prompt.format(football_player=my_player))\n",
    "print(f\"Football Player: {my_player}\\nTeam: {result}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output Parser: \n",
    "format model into structured output like JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "from langchain.prompts import PromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(model_name=\"gpt-3.5-turbo-0613\", temperature=.1)\n",
    "\n",
    "# How would you like your response structured. A fancy prompt template\n",
    "response_schemas = [\n",
    "    ResponseSchema(name=\"bad_string\", description=\"Poorly formatted user input string\"),\n",
    "    ResponseSchema(name=\"good_string\", description=\"Well formatted user input string\"),\n",
    "]\n",
    "\n",
    "#  How would you like to parse the output\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "\n",
    "# See the prompt template you created for formatting\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "print(format_instructions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "You will be given a poorly formatted string from a user. Reformat it and make sure all the words are spelled correctly.\n",
    "\n",
    "{format_instructions}\n",
    "\n",
    "% USER INPUT\n",
    "{user_input}\n",
    "\n",
    "YOUR RESPONSE:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"user_input\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions}, \n",
    "    template=template\n",
    ")\n",
    "\n",
    "promptValue = prompt.format(user_input=\"batma is brce wyn\")\n",
    "print(promptValue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_output = llm(promptValue)\n",
    "llm_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_parser.parse(llm_output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indexes - Structuring documents to LLMs can work with them"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doucments Loaders \n",
    "Easy way to import data from other sources. Shared functionality with OpenAI Plugins\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import HNLoader\n",
    "loader = HNLoader(\"https://www.reddit.com/r/antiwork/comments/14feib2/i_mean_i_need_to_sleep_as_well/\")\n",
    "data = loader.load()\n",
    "print(f\"Found {len(data)} comments\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Splitters\n",
    "when your document is too long like a book for LLM. need to split it to chunks. Text splitters help with this.\n",
    "There are many ways you could split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter \n",
    "with open('C:/Users/USER/OneDrive/Documents/ai-chatbot-3/server/test_data/test.txt') as f:\n",
    "    pg_work = f.read()\n",
    "print(f\"You have {len(pg_work)} documents\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=150,\n",
    "    chunk_overlap=20,\n",
    ")\n",
    "\n",
    "text = text_splitter.create_documents(pg_work)\n",
    "print(f\"You have {len(text)} documents\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrievers: Bind Documents with LLM. Vectorstore retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "loader = TextLoader(\"C:/Users/USER/OneDrive/Documents/ai-chatbot-3/server/test_data/test.txt\")\n",
    "document = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split text into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=50,\n",
    ")\n",
    "\n",
    "# split doc into texts\n",
    "texts = text_splitter.split_documents(document)\n",
    "\n",
    "# create embeddings\n",
    "embedding = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
    "\n",
    "# embed text\n",
    "db = FAISS.from_documents(texts, embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever()\n",
    "docs = retriever.get_relevant_documents(\"When was Software Engineering first coined?\")\n",
    "print(\"\\n\\n\".join([x.page_content[:200] for x in docs[:2]]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Memory:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Help LLM remember information.\n",
    "Memory is a bit of a loose term. It could be simple as remembering what you chatted about. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we keep it towards the chat message use case. this will be used for chat bots. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chat Message History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ChatMessageHistory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    openai_api_key=openai_api_key,\n",
    "    temperature=0.1,\n",
    "    model_name=\"gpt-3.5-turbo-0613\",\n",
    ")\n",
    "\n",
    "history = ChatMessageHistory()\n",
    "\n",
    "history.add_ai_message(\"Hello, how are you?\")\n",
    "\n",
    "history.add_user_message(\"I am doing well, how are you?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_response = chat(history.messages)\n",
    "ai_response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.add_ai_message(ai_response.content)\n",
    "history.messages"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chains\n",
    "combining LLMs calls and actions automatically. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Simple Sequemtial Chains\n",
    "    easy chains where you can use the output of an LLM as an input into another. Good for breaking up tasks and keep it focused."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import SimpleSequentialChain, LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo-0613\", temperature=.1, streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "Mention a football club that the player that user mentions is in his prime in.\n",
    "\n",
    "%USER PLAYER\n",
    "{user_player}\n",
    "\n",
    "YOUR RESPONSE:\n",
    "\"\"\"\n",
    "prompt_template  = PromptTemplate(\n",
    "    input_variables=[\"user_player\"],\n",
    "    template=template\n",
    ")\n",
    "\n",
    "player_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt_template,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "Given a football club, give a name of the famous player and give info pf his G/A in how many appearances for the club.\n",
    "\n",
    "%USER CLUB\n",
    "{user_club}\n",
    "\n",
    "YOUR RESPONSE:\n",
    "\"\"\"\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"user_club\"],\n",
    "    template=template\n",
    ")\n",
    "\n",
    "club_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt_template,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_chain = SimpleSequentialChain(\n",
    "    chains=[player_chain, club_chain]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review = overall_chain.run(input(\"Enter name of football player or club: \"))\n",
    "print(review)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Economist and Business Analyst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import SimpleSequentialChain, LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo-0613\", temperature=.1,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "You are a professional economist. You share your opinion on the topic that user provides in an economist perspective.\n",
    "\n",
    "%USER TOPIC\n",
    "{user_topic}\n",
    "\n",
    "YOUR RESPONSE:\n",
    "\"\"\"\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"user_topic\"],\n",
    "    template=template\n",
    ")\n",
    "\n",
    "economist_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt_template,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "You are a professional business analyst. You share your opinion(sometimes agree, sometimes do not agree) on what the economist said about the topic user provides in an analyst perspective.\n",
    "\n",
    "% ECONOMIST SAID\n",
    "{economist_said}\n",
    "\"\"\"\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"economist_said\"],\n",
    "    template=template\n",
    ")\n",
    "\n",
    "business_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt_template,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_chain = SimpleSequentialChain(\n",
    "    chains=[economist_chain, business_chain],\n",
    "    verbose=True,\n",
    ")\n",
    "user_input = input(\"Enter topic: \")\n",
    "review = overall_chain.run(user_input)\n",
    "print(f\"Economist: {economist_chain.run(user_input)}\")\n",
    "print(f\"Business Analyst: {review}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summerization Chain: \n",
    "easily run thru long numerous chain and get a summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo-0613\", temperature=.1)\n",
    "\n",
    "loader = TextLoader('C:/Users/USER/OneDrive/Documents/ai-chatbot-3/server/test_data/test.txt')\n",
    "documents = loader.load()\n",
    "\n",
    "# Get your splitter ready\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=700,\n",
    "    chunk_overlap=50,\n",
    ")\n",
    "\n",
    "# Split your docs into text\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "# Load your summarizer\n",
    "chain = load_summarize_chain(llm, chain_type=\"map_reduce\", verbose=True)\n",
    "chain.run(texts)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agents:\n",
    "The language model that drives the decision making.\n",
    "\n",
    "it takes in an input and returns a response to an action to take along with an action input. You can see different types of agents which are better for different use cases."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tools\n",
    "a capibility of an agent. This is an abstraction on top of a function that makes it easy to for LLMs and agents to interact with it EX: Goolge Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import load_tools, initialize_agent\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo-0613\", temperature=.1)\n",
    "serpapi_api_key = \"10f2e4c2873c46a9a7cb3bb806af9d7688f141e8f2ac293422d971b48225b136\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI should search for the current price of Bitcoin.\n",
      "Action: Search\n",
      "Action Input: \"current price of Bitcoin\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mThe Bitcoin price is $30,460.13, a change of -0.34% over the past 24 hours as of 12:00 a.m. The recent price action in Bitcoin left the tokens market ...\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now know the current price of Bitcoin.\n",
      "Final Answer: The current price of Bitcoin is $30,460.13.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "toolkit = load_tools(['serpapi'], llm=llm, serpapi_api_key=serpapi_api_key)\n",
    "agent = initialize_agent(toolkit, llm, agent=\"zero-shot-react-description\", verbose=True, return_intermediate_steps=True)\n",
    "response = agent({\n",
    "    \"input\": input(\"Enter: \"),\n",
    "}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(response[\"intermediate_steps\"], indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
